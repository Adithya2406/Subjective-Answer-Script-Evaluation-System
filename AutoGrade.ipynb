{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94112765",
   "metadata": {},
   "source": [
    "## PBL PROJECT - AutoGrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1213d71f",
   "metadata": {},
   "source": [
    "Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ec10a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, My name is AutoGrade...\n",
      "I'm an automatic subjective answer script evaluator.\n",
      "\n",
      "Select the type of input : \n",
      "1 if the student answer is in form of image.\n",
      "2 if the student is attending viva.\n",
      "Choose the approx. option : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adith\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\adith\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\adith\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we dont like engineering we joined engineeving\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello, My name is AutoGrade...\")\n",
    "print(\"I'm an automatic subjective answer script evaluator.\")\n",
    "print(\"\")\n",
    "print(\"Select the type of input : \")\n",
    "print(\"1 if the student answer is in form of image.\")\n",
    "print(\"2 if the student is attending viva.\")\n",
    "option = input(\"Choose the approx. option : \")\n",
    "\n",
    "\n",
    "if option == '1':\n",
    "    import easyocr\n",
    "    import os\n",
    "\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "    reader = easyocr.Reader(['en'])\n",
    "\n",
    "    img_path = \"a.jpg\"\n",
    "    img = img_path\n",
    "    # print(type(img_path))\n",
    "\n",
    "    result = reader.readtext(img)\n",
    "    # print(type(result))\n",
    "    # print(result)\n",
    "\n",
    "    res_list = []\n",
    "\n",
    "    for res in result:\n",
    "    #     print(res[1])\n",
    "        res_list.append(res[1])\n",
    "\n",
    "    ans=[]\n",
    "    for i in res_list:\n",
    "      ans.append(i.lower())\n",
    "\n",
    "    ans1=[]\n",
    "    for i in ans:\n",
    "      ans1 = ans1 + i.split()\n",
    "    # ans1\n",
    "\n",
    "    import nltk\n",
    "    from nltk.metrics.distance import jaccard_distance\n",
    "    from nltk.util import ngrams\n",
    "    nltk.download('words')\n",
    "    from nltk.corpus import words\n",
    "\n",
    "    correct_words = words.words()\n",
    "\n",
    "    incorrect_words = ans1\n",
    "    # print(incorrect_words)\n",
    "\n",
    "    for word in incorrect_words:\n",
    "        temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
    "                   set(ngrams(w, 2))),w)\n",
    "          for w in correct_words if w[0]==word[0]]\n",
    "        if sorted(temp, key = lambda val:val[0]) == []:\n",
    "            continue\n",
    "        else:\n",
    "            sorted(temp, key = lambda val:val[0])[0][1]\n",
    "\n",
    "    from spellchecker import SpellChecker\n",
    "    spell = SpellChecker()\n",
    "\n",
    "    misspelled = spell.unknown(ans1)\n",
    "\n",
    "    for word in misspelled:\n",
    "        if word == 'nlp':\n",
    "            ans1 += word\n",
    "        else:\n",
    "            ans1[ans1.index(word)]=spell.correction(word)\n",
    "            \n",
    "    ans1 = [x for x in ans1 if x is not None]\n",
    "    sen = \"\"\n",
    "\n",
    "    sen = \" \".join(ans1)\n",
    "    print(sen)\n",
    "    \n",
    "elif option == '2':\n",
    "    import speech_recognition as sr\n",
    "\n",
    "    # Create a recognizer instance\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "    # Set the microphone as the audio source\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Say something...\")\n",
    "\n",
    "        # Adjust for ambient noise levels\n",
    "        r.adjust_for_ambient_noise(source)\n",
    "\n",
    "        # Listen for speech and convert it to text\n",
    "        try:\n",
    "            audio = r.listen(source)\n",
    "\n",
    "            # Recognize speech using Google Speech Recognition\n",
    "            text = r.recognize_google(audio)\n",
    "\n",
    "            # Print the recognized text\n",
    "            print(\"You said:\", text)\n",
    "            sen = text\n",
    "\n",
    "            # Check if the word \"boobs\" is detected\n",
    "            if \"exit\" in text.lower():\n",
    "                print(\"Detected 'exit'. Exited\")\n",
    "                exit()\n",
    "\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio\")\n",
    "\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results; {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b36a118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we dont like engineering we joined engineeving\n"
     ]
    }
   ],
   "source": [
    "print(sen)\n",
    "full = sen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7dce7e",
   "metadata": {},
   "source": [
    "Keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9149714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dont', 'engineering', 'joined', 'engineeving']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "all_headlines = full\n",
    "stopwords = STOPWORDS\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", max_words=1000).generate(all_headlines)\n",
    "\n",
    "ans = list(wordcloud.words_)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcecc9f",
   "metadata": {},
   "source": [
    "Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c92c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joined', 'direct', 'engineering_science', 'link', 'get_together', 'coupled', 'bring_together', 'linked', 'engineering', 'organize', 'conjoin', 'united', 'join', 'orchestrate', 'connect', 'engineeving', 'unite', 'dont', 'technology', 'engine_room', 'mastermind', 'link_up', 'fall_in', 'engineer', 'applied_science', 'organise']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "syno = []\n",
    "phrase = ans\n",
    "\n",
    "for i in phrase:\n",
    "    for syn in wordnet.synsets(i):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "#             print(synonyms)\n",
    "    synonyms = list(set(synonyms))\n",
    "#     print(synonyms)\n",
    "    syno.extend(synonyms[:30])\n",
    "#     print(syno)\n",
    "#     print('\\n')\n",
    "syno = syno+ans\n",
    "syno = list(set(syno))\n",
    "print(syno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758fa4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joined', 'direct', 'engineering_science', 'link', 'get_together', 'coupled', 'bring_together', 'linked', 'engineering', 'organize', 'conjoin', 'united', 'join', 'orchestrate', 'connect', 'engineeving', 'unite', 'dont', 'technology', 'engine_room', 'mastermind', 'link_up', 'fall_in', 'engineer', 'applied_science', 'organise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem_ans = []\n",
    "for word in syno:\n",
    "    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "    lem_ans.append(lemmatized_word)\n",
    "    \n",
    "print(lem_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6432ea5e",
   "metadata": {},
   "source": [
    "Answer key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aeabc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'involves', 'developing', 'algorithms', 'techniques', 'enable', 'computers', 'understand', 'analyze', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "all_headlines = ''' NLP involves developing algorithms and techniques to enable \n",
    "computers to understand and analyze human language'''\n",
    "\n",
    "stopwords = STOPWORDS\n",
    "wordcloud = WordCloud(stopwords=stopwords, max_words=15).generate(all_headlines)\n",
    "answer_key = list(wordcloud.words_)\n",
    "print(answer_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36d035f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'involves', 'developing', 'algorithm', 'technique', 'enable', 'computer', 'understand', 'analyze', 'human', 'language']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem_ans_key = []\n",
    "for word in answer_key:\n",
    "    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "    lem_ans_key.append(lemmatized_word)\n",
    "    \n",
    "print(lem_ans_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d57ee",
   "metadata": {},
   "source": [
    "Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac11131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "count = 0 \n",
    "marks = 0\n",
    "\n",
    "for i in lem_ans_key:\n",
    "    if i in lem_ans:\n",
    "        count += 1\n",
    "        continue\n",
    "        \n",
    "marks = (count/len(lem_ans_key))*10\n",
    "print(math.ceil(marks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
